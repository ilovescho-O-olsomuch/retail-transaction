## æ•°æ®åˆ†å±‚ä»¥åŠèšç±»

### ä¸ºä»€ä¹ˆé€‰å– RFM + K-means å¯¹å®¢æˆ·è¿›è¡Œåˆ†ç¾¤æ ‡ç­¾ï¼Ÿ

#### RFM çš„æ„ä¹‰
R ä»£è¡¨ç€ **Recency**ï¼Œè¡¨ç¤ºå®¢æˆ·å¤šä¹…æ²¡æœ‰è´­ä¹°ä¸œè¥¿ï¼Œå€¼è¶Šå°åˆ™å®¢æˆ·è¶Šæ´»è·ƒã€‚  
F ä»£è¡¨ç€ **Frequency**ï¼Œè¡¨ç¤ºå®¢æˆ·çš„è´­ä¹°æ¬¡æ•°ã€‚  
M ä»£è¡¨ç€ **Monetary**ï¼Œè¡¨ç¤ºå®¢æˆ·çš„æ€»æ¶ˆè´¹é‡‘é¢ã€‚

åœ¨è¿™ä¸‰ä¸ªæŒ‡æ ‡é‡Œï¼š
- **R** å¯ä¾èµ– `TransactionDate` å­—æ®µè®¡ç®—  
- **F** é€šè¿‡ `CustomerID` åˆ†ç»„å `count()` è®¡ç®—  
- **M** ä¾èµ– `TotalAmount` å­—æ®µ

RFM è¿›è¡Œå®¢æˆ·åˆ†ç¾¤çš„ä¼˜åŠ¿åœ¨äº **ç®€å•ç›´è§‚**ï¼Œå¯æ¸…æ™°æè¿°å®¢æˆ·çš„ä»·å€¼ã€‚

---

### ä¸ºä»€ä¹ˆé€‰æ‹© K-Means è¿›è¡Œèšç±»ï¼Ÿ
K-Means ä¸»è¦ç”¨äºå¯»æ‰¾æ•°æ®ä¸­çš„è‡ªç„¶åˆ†ç±»ï¼Œ**è®¡ç®—é«˜æ•ˆ**ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶èƒ½ **è‡ªåŠ¨åˆ’åˆ†ç¾¤ç»„**ã€‚

---

## å®ç° RFM åˆ†å±‚ä»£ç 
# RFM åˆ†æå®Œæ•´ä»£ç åŠè®²è§£

## 1. å¯¼å…¥ Python åº“
```python
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
```

æœ¬éƒ¨åˆ†å¯¼å…¥äº†å¿…è¦çš„ Python åº“ï¼š
- `pandas` ç”¨äºæ•°æ®å¤„ç†ã€‚
- `numpy` ç”¨äºæ•°å€¼è®¡ç®—ã€‚
- `datetime` å¤„ç†æ—¶é—´æ•°æ®ã€‚
- `StandardScaler` è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ã€‚
- `KMeans` è¿›è¡Œ K-Means èšç±»ã€‚

## 2. è¯»å– CSV æ–‡ä»¶
```python
file_path = r"C:\Users\32165\Desktop\retail\cleaned_retail.csv"
df = pd.read_csv(file_path, dtype={"CustomerID": str}, parse_dates=["TransactionDate"])
df["OriginalOrder"] = df.index  # è®°å½•åŸå§‹ç´¢å¼•ï¼Œä¾¿äºåç»­æ’åºæ¢å¤
```

- `dtype={"CustomerID": str}`ï¼šç¡®ä¿ `CustomerID` ä½œä¸ºå­—ç¬¦ä¸²å¤„ç†ï¼Œé¿å…æ•°å€¼æ ¼å¼åŒ–é—®é¢˜ã€‚
- `parse_dates=["TransactionDate"]`ï¼šå°† `TransactionDate` åˆ—è§£æä¸ºæ—¥æœŸæ ¼å¼ã€‚
- è®°å½• `OriginalOrder`ï¼Œä»¥ä¾¿åç»­æ¢å¤åŸå§‹é¡ºåºã€‚

## 3. è®¡ç®— R æŒ‡æ ‡çš„åŸºå‡†æ—¥æœŸ
```python
max_date = df["TransactionDate"].max()
if max_date.month == 12:
    cutoff_date = datetime(max_date.year + 1, 1, 1)
else:
    cutoff_date = datetime(max_date.year, max_date.month + 1, 1)
print(f"\nåŸºå‡†æ—¥æœŸï¼ˆæ¬¡æœˆ1æ—¥ï¼‰ï¼š{cutoff_date.strftime('%Y-%m-%d')}")
```

- `max_date` è·å–æ•°æ®é›†ä¸­æœ€æ–°çš„äº¤æ˜“æ—¥æœŸã€‚
- `cutoff_date` è®¾å®šä¸º `max_date` æ¬¡æœˆ 1 å·ï¼Œä½œä¸º `Recency` è®¡ç®—çš„åŸºå‡†æ—¥æœŸã€‚

## 4. è®¡ç®— RFM æŒ‡æ ‡
```python
rfm = df.groupby("CustomerID", as_index=False).agg(
    Recency=("TransactionDate", lambda x: (cutoff_date - x.max()).days),
    Frequency=("TransactionDate", "count"),
    Monetary=("TotalAmount", "sum")
)
```

- `Recency` è®¡ç®—å®¢æˆ·æœ€è¿‘ä¸€æ¬¡äº¤æ˜“è·ç¦»åŸºå‡†æ—¥æœŸçš„å¤©æ•°ã€‚
- `Frequency` ç»Ÿè®¡æ¯ä¸ªå®¢æˆ·çš„äº¤æ˜“æ¬¡æ•°ã€‚
- `Monetary` è®¡ç®—å®¢æˆ·çš„æ€»æ¶ˆè´¹é‡‘é¢ã€‚

## 5. æ•°æ®æ ‡å‡†åŒ–
```python
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[["Recency", "Frequency", "Monetary"]])
rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=["Recency", "Frequency", "Monetary"])
```

- `StandardScaler` æ ‡å‡†åŒ–æ•°æ®ï¼Œä½¿å…¶å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼Œé¿å…ç‰¹å¾é‡çº²å½±å“èšç±»ã€‚

## 6. K-Means èšç±»
```python
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
rfm["Cluster"] = kmeans.fit_predict(rfm_scaled_df)
cluster_means = rfm.groupby("Cluster")[["Recency", "Frequency", "Monetary"]].mean()
print("\nå„ç°‡ RFM å¹³å‡å€¼ï¼š")
print(cluster_means)
```

- `n_clusters=4`ï¼šæŒ‡å®š 4 ä¸ªèšç±»ç±»åˆ«ã€‚
- `fit_predict()` è¿›è¡Œ K-Means èšç±»ï¼Œå¹¶å°†ç»“æœå­˜å…¥ `Cluster` åˆ—ã€‚
- `cluster_means` è®¡ç®—æ¯ä¸ªèšç±»çš„å¹³å‡ RFM å€¼ã€‚

## 7. å®šä¹‰åˆ†ç¾¤æ ‡ç­¾
```python
cluster_labels = {}
sorted_clusters = cluster_means.sort_values(by=["Monetary", "Frequency", "Recency"], ascending=[False, False, True]).index
cluster_labels[sorted_clusters[0]] = "æµå¤±å®¢æˆ·"
cluster_labels[sorted_clusters[1]] = "ä¸€èˆ¬å®¢æˆ·"
cluster_labels[sorted_clusters[2]] = "é«˜ä»·å€¼å®¢æˆ·"
cluster_labels[sorted_clusters[3]] = "æ½œåŠ›å®¢æˆ·"
rfm["Cluster_Label"] = rfm["Cluster"].map(cluster_labels)
```

- æŒ‰ `Monetary`ã€`Frequency`ã€`Recency` è¿›è¡Œæ’åºï¼Œç¡®å®šç¾¤ç»„æ ‡ç­¾ã€‚
- `map()` è¿›è¡Œæ ‡ç­¾æ˜ å°„ã€‚

## 8. æ¢å¤åŸå§‹æ•°æ®æ’åˆ—é¡ºåº
```python
rfm = df[["CustomerID", "OriginalOrder"]].drop_duplicates().merge(rfm, on="CustomerID", how="left")
rfm = rfm.sort_values("OriginalOrder").drop(columns=["OriginalOrder"])
```

- `drop_duplicates()` ç¡®ä¿ `CustomerID` å”¯ä¸€ã€‚
- `merge()` ç»“åˆåŸå§‹ç´¢å¼•ä»¥æ¢å¤é¡ºåºã€‚
- `sort_values("OriginalOrder")` é‡æ–°æŒ‰åŸå§‹é¡ºåºæ’åˆ—ã€‚

## 9. ç»Ÿè®¡åˆ†æ
```python
original_counts = rfm["Cluster_Label"].value_counts()
unique_counts = rfm.drop_duplicates(subset=["CustomerID"])["Cluster_Label"].value_counts()
print("\nğŸ”¹ åŸå§‹ç»Ÿè®¡ï¼ˆå¯èƒ½æœ‰é‡å¤ CustomerIDï¼‰ï¼š")
print(original_counts)
print("\nğŸ”¹ å»é‡åç»Ÿè®¡ï¼ˆæ¯ä¸ª CustomerID ä»…è®¡ç®—ä¸€æ¬¡ï¼‰ï¼š")
print(unique_counts)
output_file_path = r"C:\Users\32165\Desktop\rfm_result.csv"
rfm.drop_duplicates(subset=["CustomerID"]).to_csv(output_file_path, index=False, encoding="utf-8-sig")
print(f"\nâœ… RFM ç»“æœå·²ä¿å­˜è‡³: {output_file_path}")
```

- `value_counts()` è®¡ç®—æ¯ä¸ªåˆ†ç¾¤çš„å®¢æˆ·æ•°é‡ã€‚
- `drop_duplicates(subset=["CustomerID"])` ç¡®ä¿ `CustomerID` å”¯ä¸€ã€‚
- ç»“æœå­˜å…¥ `rfm_result.csv`ã€‚

## 10. ç»Ÿè®¡ç»“æœç¤ºä¾‹
```yaml
ğŸ”¹ åŸå§‹ç»Ÿè®¡ï¼ˆå¯èƒ½æœ‰é‡å¤ CustomerIDï¼‰ï¼š
æ½œåŠ›å®¢æˆ·     35057  
é«˜ä»·å€¼å®¢æˆ·    34958  
æµå¤±å®¢æˆ·     20579  
ä¸€èˆ¬å®¢æˆ·      9406  

ğŸ”¹ å»é‡åç»Ÿè®¡ï¼ˆæ¯ä¸ª CustomerID ä»…è®¡ç®—ä¸€æ¬¡ï¼‰ï¼š
æ½œåŠ›å®¢æˆ·     35057  
é«˜ä»·å€¼å®¢æˆ·    34958  
æµå¤±å®¢æˆ·     20579  
ä¸€èˆ¬å®¢æˆ·      4621  
```

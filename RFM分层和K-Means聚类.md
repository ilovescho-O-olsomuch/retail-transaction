# æ•°æ®åˆ†å±‚ä»¥åŠèšç±»

## ä¸ºä»€ä¹ˆé€‰å– RFM + K-means å¯¹å®¢æˆ·è¿›è¡Œåˆ†ç¾¤æ ‡ç­¾ï¼Ÿ

### RFM çš„æ„ä¹‰
R ä»£è¡¨ç€ **Recency**ï¼Œè¡¨ç¤ºå®¢æˆ·å¤šä¹…æ²¡æœ‰è´­ä¹°ä¸œè¥¿ï¼Œå€¼è¶Šå°åˆ™å®¢æˆ·è¶Šæ´»è·ƒã€‚  
F ä»£è¡¨ç€ **Frequency**ï¼Œè¡¨ç¤ºå®¢æˆ·çš„è´­ä¹°æ¬¡æ•°ã€‚  
M ä»£è¡¨ç€ **Monetary**ï¼Œè¡¨ç¤ºå®¢æˆ·çš„æ€»æ¶ˆè´¹é‡‘é¢ã€‚

åœ¨è¿™ä¸‰ä¸ªæŒ‡æ ‡é‡Œï¼š
- **R** å¯ä¾èµ– `TransactionDate` å­—æ®µè®¡ç®—  
- **F** é€šè¿‡ `CustomerID` åˆ†ç»„å `count()` è®¡ç®—  
- **M** ä¾èµ– `TotalAmount` å­—æ®µ

RFM è¿›è¡Œå®¢æˆ·åˆ†ç¾¤çš„ä¼˜åŠ¿åœ¨äº **ç®€å•ç›´è§‚**ï¼Œå¯æ¸…æ™°æè¿°å®¢æˆ·çš„ä»·å€¼ã€‚

---

## ä¸ºä»€ä¹ˆé€‰æ‹© K-Means è¿›è¡Œèšç±»ï¼Ÿ
K-Means ä¸»è¦ç”¨äºå¯»æ‰¾æ•°æ®ä¸­çš„è‡ªç„¶åˆ†ç±»ï¼Œ**è®¡ç®—é«˜æ•ˆ**ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶èƒ½ **è‡ªåŠ¨åˆ’åˆ†ç¾¤ç»„**ã€‚

---

## å®ç° RFM åˆ†å±‚ä»£ç 

### å¯¼å…¥ Python åº“
```python
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
```

### è¯»å– CSV æ–‡ä»¶
```python
file_path = r"C:\Users\32165\Desktop\cleaned_retail.csv"
df = pd.read_csv(file_path, dtype={"CustomerID": str}, parse_dates=["TransactionDate"])

# è®°å½• CustomerID åŸå§‹é¡ºåºï¼ˆç”¨äºåç»­æ¢å¤ï¼‰
df["OriginalOrder"] = df.index  # è®°å½•åŸå§‹ç´¢å¼•ï¼Œä¾¿äºåç»­æ’åºæ¢å¤
```

### è®¡ç®— R æŒ‡æ ‡çš„åŸºå‡†æ—¥æœŸ
```python
# è·å–æ•°æ®ä¸­çš„æœ€å¤§äº¤æ˜“æ—¥æœŸ
max_date = df["TransactionDate"].max()
if max_date.month == 12:
    cutoff_date = datetime(max_date.year + 1, 1, 1)
else:
    cutoff_date = datetime(max_date.year, max_date.month + 1, 1)

# è¾“å‡ºåŸºå‡†æ—¥æœŸ
print(f"\nåŸºå‡†æ—¥æœŸï¼ˆæ¬¡æœˆ1æ—¥ï¼‰ï¼š{cutoff_date.strftime('%Y-%m-%d')}")
```

### è®¡ç®— RFM æŒ‡æ ‡
```python
rfm = df.groupby("CustomerID", as_index=False).agg(
    Recency=("TransactionDate", lambda x: (cutoff_date - x.max()).days),  # æœ€è¿‘è´­ä¹°å¤©æ•°
    Frequency=("TransactionDate", "count"),  # è´­ä¹°æ¬¡æ•°
    Monetary=("TotalAmount", "sum")  # æ€»æ¶ˆè´¹é‡‘é¢
)
```

### æ•°æ®æ ‡å‡†åŒ–
```python
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[["Recency", "Frequency", "Monetary"]])

# è½¬æ¢ä¸º DataFrame 
rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=["Recency", "Frequency", "Monetary"])
```

### K-Means èšç±»
```python
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)

# è¿›è¡Œèšç±»ï¼Œå¹¶å°†èšç±»æ ‡ç­¾èµ‹å€¼ç»™ RFM æ•°æ®
rfm["Cluster"] = kmeans.fit_predict(rfm_scaled_df)

# å®šä¹‰åˆ†ç¾¤æ ‡ç­¾
cluster_labels = {
    0: "æµå¤±å®¢æˆ·",
    1: "ä¸€èˆ¬å®¢æˆ·",
    2: "é«˜ä»·å€¼å®¢æˆ·",
    3: "æ½œåŠ›å®¢æˆ·"
}

# æ˜ å°„åˆ†ç¾¤æ ‡ç­¾
rfm["Cluster_Label"] = rfm["Cluster"].map(cluster_labels)
```

### æ¢å¤åŸå§‹æ•°æ®æ’åˆ—é¡ºåº
```python
# é€šè¿‡åˆå¹¶æ¢å¤ CustomerID åŸå§‹é¡ºåº
rfm = df[["CustomerID", "OriginalOrder"]].drop_duplicates().merge(rfm, on="CustomerID", how="left")

# æŒ‰åŸå§‹é¡ºåºæ’åºï¼Œå¹¶åˆ é™¤è¾…åŠ©åˆ—
rfm = rfm.sort_values("OriginalOrder").drop(columns=["OriginalOrder"])
```

### RFM+K-means è¾“å‡ºæ•ˆæœ
![RFM+K-means è¾“å‡ºæ•ˆæœ](https://github.com/ilovescho-O-olsomuch/retail-transaction/blob/main/RFM%2BKMEANS.png)

### ç»Ÿè®¡åˆ†æ
```python
# åŸå§‹åˆ†ç¾¤ç»Ÿè®¡
original_counts = rfm["Cluster_Label"].value_counts()

# å»é‡åçš„åˆ†ç¾¤ç»Ÿè®¡
unique_counts = rfm.drop_duplicates(subset=["CustomerID"])["Cluster_Label"].value_counts()

print("\nğŸ”¹ åŸå§‹ç»Ÿè®¡ï¼ˆå¯èƒ½æœ‰é‡å¤ CustomerIDï¼‰ï¼š")
print(original_counts)

print("\nğŸ”¹ å»é‡åç»Ÿè®¡ï¼ˆæ¯ä¸ª CustomerID ä»…è®¡ç®—ä¸€æ¬¡ï¼‰ï¼š")
print(unique_counts)

# æ£€æŸ¥å»é‡å‰åçš„æ€»äººæ•°æ˜¯å¦ä¸€è‡´
print("\nâœ… æ€»äººæ•°å¯¹æ¯”ï¼ˆå»é‡å‰ vs. å»é‡åï¼‰ï¼š")
print(f"å»é‡å‰æ€»äººæ•°ï¼š{original_counts.sum()}")
print(f"å»é‡åæ€»äººæ•°ï¼š{unique_counts.sum()}")
```

### ç»Ÿè®¡ç»“æœç¤ºä¾‹
```
ğŸ”¹ åŸå§‹ç»Ÿè®¡ï¼ˆå¯èƒ½æœ‰é‡å¤ CustomerIDï¼‰ï¼š
æ½œåŠ›å®¢æˆ·ï¼š35057  
æµå¤±å®¢æˆ·ï¼š34958  
ä¸€èˆ¬å®¢æˆ·ï¼š20579  
é«˜ä»·å€¼å®¢æˆ·ï¼š9406  

ğŸ”¹ å»é‡åç»Ÿè®¡ï¼ˆæ¯ä¸ª CustomerID ä»…è®¡ç®—ä¸€æ¬¡ï¼‰ï¼š
æ½œåŠ›å®¢æˆ·ï¼š12035  
æµå¤±å®¢æˆ·ï¼š11875  
ä¸€èˆ¬å®¢æˆ·ï¼š8543  
é«˜ä»·å€¼å®¢æˆ·ï¼š4120  

âœ… æ€»äººæ•°å¯¹æ¯”ï¼ˆå»é‡å‰ vs. å»é‡åï¼‰ï¼š
å»é‡å‰æ€»äººæ•°ï¼š100000  
å»é‡åæ€»äººæ•°ï¼š36573  
